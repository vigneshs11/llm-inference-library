# ğŸ§  LLM Inference Library

This project is a hands-on sandbox to run and benchmark transformer models (like DistilBERT) locally on GPU using:

- ğŸ§ª PyTorch
- ğŸ“¦ ONNX Runtime
- âš¡ NVIDIA GPU (CUDA)

---

## âœ… Week 1 Goals

> "I optimized a transformer model on my laptop GPU â€” here's what I learned."

- [x] Setup WSL2 + Ubuntu + NVIDIA drivers
- [x] Run inference with Hugging Face + PyTorch on GPU
- [x] Export transformer model to ONNX
- [x] Run ONNX inference (CPU fallback for now)
- [x] Compare logits & speed vs PyTorch

---

## ğŸ“ Project Structure

```bash
llm-inference-library/
â”œâ”€â”€ export_onnx.py       # Convert PyTorch model to ONNX
â”œâ”€â”€ run_distilbert.py    # Run inference using PyTorch
â”œâ”€â”€ run_onnx.py          # Run inference using ONNX Runtime
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
